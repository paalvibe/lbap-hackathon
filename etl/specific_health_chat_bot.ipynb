{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c0a10a-f1e9-47b3-9dae-1bb0a03b0d75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Simple RAG Example using Vector Search and the Foundation Model API\n",
    "\n",
    "[Databricks Vector Search](https://docs.databricks.com/en/generative-ai/vector-search.html) is a vector database built into Databricks that offers straightforward integration with the [Databricks Foundation Model API](https://docs.databricks.com/en/machine-learning/foundation-models/index.html) (FMAPI) embedding models.\n",
    "\n",
    "Retrieval-augmented generation (RAG) is one of the most popular application architectures for creating natural-language interfaces for people to interact with an organization's data. This notebook builds a very simple RAG application, with the following steps:\n",
    "\n",
    "1. Set up a vector index and configure it to automatically use an embedding model from the FMAPI to generate embeddings.\n",
    "1. Load some text data into the vector database\n",
    "1. Query the database\n",
    "1. Build a prompt for an LLM from the query results\n",
    "1. Query an LLM via the FMAPI, using that prompt\n",
    "\n",
    "To learn more about how Databricks Vector Search works, see the documentation [here](https://docs.databricks.com/en/generative-ai/vector-search.html#how-does-vector-search-work).\n",
    "\n",
    "For more details on querying models via the Foundation Model APIs, see the documentation [here](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html#query-foundation-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1102d54c-03f9-4501-9e69-3c51d94a9ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "First, we will install the necessary libraries and set up a temporary catalog/schema/table for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43421aa6-1401-47d1-ac80-1d255e8db479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall databricks-vectorsearch databricks-genai-inference\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567a035b-23ec-4f82-ac33-b6058bedecf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define catalog, table, endpoint, and index names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de74cb6-f62b-4fe6-aae9-2a0b9b19481f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "DB='vs_demo'\n",
    "SYMPTOMS_SOURCE_TABLE_NAME = \"symptoms\"\n",
    "SYMPTOMS_SOURCE_TABLE_FULLNAME = f\"{CATALOG}.{DB}.{SYMPTOMS_SOURCE_TABLE_NAME}\"\n",
    "PROCEDURES_SOURCE_TABLE_NAME = \"procedures\"\n",
    "PROCEDURES_SOURCE_TABLE_FULLNAME = f\"{CATALOG}.{DB}.{PROCEDURES_SOURCE_TABLE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95cd38e-fac3-41a3-979d-ff329879e84c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Catalog, Schema, and Table\n",
    "\n",
    "A Databricks Vector Search Index is created from a Delta Table. The source Delta Table includes the data we ultimately want to index and search with the vector database. In this cell, we create the catalog, schema, and source table from which we will create the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74fa939c-ef24-4ae3-8c38-4373e255c88b",
     "showTitle": true,
     "title": "Pyspark Schema Volume Table Setup"
    }
   },
   "outputs": [],
   "source": [
    "# Set up schema/volume/table\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{DB}\")\n",
    "spark.sql(\n",
    "    f\"\"\"CREATE TABLE IF NOT EXISTS {SYMPTOMS_SOURCE_TABLE_NAME} (\n",
    "        id STRING,\n",
    "        text STRING,\n",
    "        date DATE,\n",
    "        title STRING\n",
    "    )\n",
    "    USING delta \n",
    "    TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a92509-0aa3-49aa-b4a8-f38bb75c8522",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up procedures source table\n",
    "spark.sql(\n",
    "    f\"\"\"CREATE TABLE IF NOT EXISTS {PROCEDURES_SOURCE_TABLE_NAME} (\n",
    "        id STRING,\n",
    "        text STRING,\n",
    "        date DATE,\n",
    "        title STRING\n",
    "    )\n",
    "    USING delta \n",
    "    TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4e7db85-65c7-4f1a-b23f-a55203275800",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set up the Vector Database\n",
    "Next, we set up the vector database. There are three key steps:\n",
    "1. Initialize the vector search client\n",
    "2. Create the endpoint\n",
    "3. Create the index using the source Delta table we created earlier and the `bge-large-en` embeddings model from the Foundation Model API\n",
    "\n",
    "### Initialize the Vector Search Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "638fb80d-9774-418a-8073-53ea831b6b5b",
     "showTitle": true,
     "title": "Vector Search Client Initialization"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73400ec-2e4e-4f5b-b372-1401a43a3f71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create the Endpoint\n",
    "\n",
    "The cell below will check if the endpoint already exists and create it if it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e05dcf9-7678-4622-9ef4-d3e1029fbc61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VS_ENDPOINT_NAME = 'hackathon'\n",
    "\n",
    "if vsc.list_endpoints().get('endpoints') == None or not VS_ENDPOINT_NAME in [endpoint.get('name') for endpoint in vsc.list_endpoints().get('endpoints')]:\n",
    "    print(f\"Creating new Vector Search endpoint named {VS_ENDPOINT_NAME}\")\n",
    "    vsc.create_endpoint(VS_ENDPOINT_NAME)\n",
    "else:\n",
    "    print(f\"Endpoint {VS_ENDPOINT_NAME} already exists.\")\n",
    "\n",
    "vsc.wait_for_endpoint(VS_ENDPOINT_NAME, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "869939f1-c211-4973-a3be-c99d72beea98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create the Vector Index\n",
    "\n",
    "Now we can create the index over the Delta table we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4f2157-f68c-4494-b525-c9c074294fc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_index(*, VS_INDEX_NAME, source_table_fullname):\n",
    "    VS_INDEX_FULLNAME = f\"{CATALOG}.{DB}.{VS_INDEX_NAME}\"\n",
    "\n",
    "    if not VS_INDEX_FULLNAME in [index.get(\"name\") for index in vsc.list_indexes(VS_ENDPOINT_NAME).get('vector_indexes', [])]:\n",
    "        try:\n",
    "            # set up an index with managed embeddings\n",
    "            print(\"Creating Vector Index...\")\n",
    "            i = vsc.create_delta_sync_index_and_wait(\n",
    "                endpoint_name=VS_ENDPOINT_NAME,\n",
    "                index_name=VS_INDEX_FULLNAME,\n",
    "                source_table_name=source_table_fullname,\n",
    "                pipeline_type=\"TRIGGERED\",\n",
    "                primary_key=\"id\",\n",
    "                embedding_source_column=\"text\",\n",
    "                embedding_model_endpoint_name=\"databricks-bge-large-en\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"INTERNAL_ERROR\" in str(e):\n",
    "                # Check if the index exists after the error occurred\n",
    "                if VS_INDEX_FULLNAME in [index.get(\"name\") for index in vsc.list_indexes(VS_ENDPOINT_NAME).get('vector_indexes', [])]:\n",
    "                    print(f\"Index {VS_INDEX_FULLNAME} has been created.\")\n",
    "                else:\n",
    "                    raise e\n",
    "            else:\n",
    "                raise e\n",
    "    else:\n",
    "        print(f\"Index {VS_INDEX_FULLNAME} already exists.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3173bf07-4917-410c-a581-fde7fab801e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " There are a few key points to note about the specific configuration we used in this case:\n",
    "- We used `pipeline_type=\"TRIGGERED\"`. This requires us to use the index's `sync()` method to manually sync the source Delta table with the index. We could, alternatively, use `pipeline_type=\"CONTINUOUS\"` which will automatically keep the index in sync with the source table with only seconds of latency. This approach is more costly, though, as a compute cluster must be provisioned for the continuous sync streaming pipeline.\n",
    "- We specified `embedding_model_endpoint_name=\"databricks-bge-large-en\"`. We can use any embedding model available via model serving; this is the name of the pay-per-token Foundation Model API version of `databricks-bge-large-en`. By passing an `embedding_source_column` and `embedding_model_endpoint_name`, we configure the index such that it will automatically use the model to generate embeddings for the texts in the `text` column of the source table. We do not need to manually generate embeddings.\n",
    "\n",
    "  If, however, we did want to manage embeddings manually, we could include the following arguments instead:\n",
    "\n",
    "  ```\n",
    "    embedding_vector_column=\"<embedding_column>\",\n",
    "    embedding_dimension=<embedding_dimension>\n",
    "  ```\n",
    "\n",
    "  In the latter approach, we include a column for embeddings in the source delta table and embeddings are *not* computed automatically from the text column.\n",
    "\n",
    "## Set up some example texts\n",
    "\n",
    "Now we set up some example texts to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393e6e7c-20cd-49bb-a3c0-2119735fcfa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Not sure why this was needed, but create_index threw an error about data feed must be enabled\n",
    "ALTER TABLE workspace.vs_demo.symptoms SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9623dd50-0348-4154-9524-89bfe699d97a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE workspace.vs_demo.procedures SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14ce5a7a-bd7a-43af-9af5-7793f7f81f08",
     "showTitle": true,
     "title": "Python Delta Sync Index Setup"
    }
   },
   "outputs": [],
   "source": [
    "SYMPTOMS_INDEX = \"symptoms_index_demo_small\"\n",
    "VS_SYMPTOMS_INDEX_FULLNAME = f\"{CATALOG}.{DB}.{SYMPTOMS_INDEX}\"\n",
    "create_index(VS_INDEX_NAME=SYMPTOMS_INDEX, source_table_fullname=SYMPTOMS_SOURCE_TABLE_FULLNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8ae990-d8c5-46d8-86e4-cd5e7cb6e313",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PROCEDURES_INDEX = \"procedures_index_demo_small\"\n",
    "VS_PROCEDURES_INDEX_FULLNAME = f\"{CATALOG}.{DB}.{PROCEDURES_INDEX}\"\n",
    "create_index(VS_INDEX_NAME=PROCEDURES_INDEX, source_table_fullname=PROCEDURES_SOURCE_TABLE_FULLNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df4b065c-1de5-495c-af06-0ccf53c55855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Chunk the texts\n",
    "Typically, when using a vector database for retrieval-augmented generation (RAG) tasks, we break the texts apart into smaller (and sometimes overlapping) chunks in order to return focused and relevant information without returning an excessive amount of text.\n",
    "\n",
    "In the code below, we break the sample texts above into shorter overlapping text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12df246b-2934-457c-bf57-27628ba69cb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        end = index + chunk_size\n",
    "        while end < len(words) and not re.match(r'.*[.!?]\\s*$', words[end]):\n",
    "            end += 1\n",
    "        chunk = ' '.join(words[index:end+1])\n",
    "        chunks.append(chunk)\n",
    "        index += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def make_chunks(documents):\n",
    "    chunks = []\n",
    "\n",
    "    for document in documents:\n",
    "        print(\"document:\" + repr(document))\n",
    "        for i, c in enumerate(chunk_text(document[\"text\"], 150, 25)):\n",
    "            chunk = {}\n",
    "            chunk[\"text\"] = c\n",
    "            chunk[\"title\"] = document[\"title\"]\n",
    "            chunk[\"date\"] = document[\"date\"]\n",
    "            chunk[\"id\"] = document[\"title\"] + \"_\" + str(i)\n",
    "\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "122f2253-f981-48c0-b5af-f16a3458e0c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Insert the text chunks into the source delta table\n",
    "\n",
    "Now we save the chunks, along with some metadata (a document title, date, and a unique id) to the source delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f0388ab-064e-47dd-b8eb-3ca17ba53fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, DateType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def write_chunks_to_table(*, table, chunks):\n",
    "    if chunks:\n",
    "        result_df = spark.createDataFrame(chunks, schema=schema)\n",
    "        result_df.write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "            table\n",
    "        )\n",
    "\n",
    "def chunk_and_write(*, src_tbl, src_title_col, src_txt_col, dst_tbl):\n",
    "    documents = []\n",
    "\n",
    "    df = table(src_tbl).select(\n",
    "        F.col(src_title_col).alias(\"title\"),\n",
    "        F.col(src_txt_col).alias(\"text\"),\n",
    "    )\n",
    "    valid_df = (df\n",
    "                .where(F.col(\"text\").isNotNull())\n",
    "                .where(F.col(\"title\").isNotNull())\n",
    "    )\n",
    "    rows = valid_df.collect()\n",
    "    for row in rows:\n",
    "        d = row.asDict()\n",
    "        documents.append({\n",
    "            \"text\": d[\"text\"],\n",
    "            \"title\": d[\"title\"], \n",
    "            \"date\": datetime.strptime(\"2024-01-16\", \"%Y-%m-%d\"),\n",
    "        })\n",
    "    chunks = make_chunks(documents)\n",
    "    print(\"num chunks: \" + repr(len(chunks)))\n",
    "    write_chunks_to_table(table=dst_tbl, chunks=chunks)\n",
    "    print(f\"chunks written to table {dst_tbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c770b77e-f23c-4ebe-a75b-17c6641404bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DELETE from workspace.vs_demo.symptoms;\n",
    "DELETE from workspace.vs_demo.procedures;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b073fbca-8778-465f-9191-6e293a41ea60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunk_and_write(src_tbl=\"workspace.default.aienrichmentbriefer\",\n",
    "                src_title_col=\"CODE\",\n",
    "                src_txt_col=\"conditionDescriptionAndSymptomsAI\",\n",
    "#                 src_title_col=\"code\",\n",
    "#                 src_txt_col=\"LONG DESCRIPTION (VALID ICD-10 FY2024)\",\n",
    "                dst_tbl=SYMPTOMS_SOURCE_TABLE_FULLNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a734d1df-9747-4932-89ab-bd27cf394a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunk_and_write(src_tbl=\"workspace.default.procedure_enhanced\",\n",
    "                src_title_col=\"code\",\n",
    "                src_txt_col=\"longer_description\",\n",
    "                dst_tbl=PROCEDURES_SOURCE_TABLE_FULLNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517ed55b-11f0-4ca0-8cb3-26a6713c31bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Sync the Vector Search Index\n",
    "Because we specified `pipeline_type=\"TRIGGERED\"` when configuring the Delta Index, we still need to manually tell the index to sync with the delta table. This will take a few minutes.\n",
    "\n",
    "This will not work if the index is not ready yet. We use the `wait_until_ready` method to wait until the index is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e8cb67-38a5-4dd6-9bc7-b31ef76d4c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VS_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80887325-5542-4387-a5be-c537e537ca1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sync symptoms\n",
    "symptoms_index = vsc.get_index(endpoint_name=VS_ENDPOINT_NAME,\n",
    "                      index_name=VS_SYMPTOMS_INDEX_FULLNAME)\n",
    "symptoms_index.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02885aa4-dfb1-4ac7-97ff-860247b086b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sync procedures\n",
    "procedures_index = vsc.get_index(endpoint_name=VS_ENDPOINT_NAME,\n",
    "                      index_name=VS_PROCEDURES_INDEX_FULLNAME)\n",
    "procedures_index.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da431c89-0d73-4769-ae35-4fd900528263",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Query the Vector Indexes\n",
    "\n",
    "Now that we have added our text chunks to the source delta table and synced it with the Vector Search index, we're ready to query the index! We do this with the `index.similarity_search()` method.\n",
    "\n",
    "The `columns` argument takes a list of the columns we want returned; in this case, we request the text and title columns.\n",
    "\n",
    "**NOTE**: If the cell below does not return any results, wait a couple of minutes and try again. The index may still be syncing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c35aa051-31a3-442a-aa2f-b14c8648e7bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chats = [\n",
    "    [\"I have pains in the upper right abdomen, and I have been losing weight. What could it be?\"],\n",
    "    [\"I have abdominal pain. Can it be fever?\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2592fbd-8e26-4304-a6e8-7de1a522d489",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# symptoms query\n",
    "question = chats[0][0]\n",
    "symptoms_index.similarity_search(columns=[\"text\", \"title\"],\n",
    "                        query_text=question,\n",
    "                        num_results = 3, score_threshold=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273fe41e-27da-4b44-95c2-3ee8ae7a8748",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# procedures query\n",
    "procedures_index.similarity_search(columns=[\"text\", \"title\"],\n",
    "                        query_text=question,\n",
    "                        num_results = 3,\n",
    "                        score_threshold=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30d0adf6-37d6-4be2-ae8d-992638765ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answering Questions without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f539c95c-7bfd-4600-b39c-169c504406ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_genai_inference import ChatSession\n",
    "\n",
    "chat = ChatSession(model=\"databricks-meta-llama-3-70b-instruct\",\n",
    "                   system_message=\"You are a helpful hospital assistant.\",\n",
    "                   max_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571f80ab-0d91-4965-b355-7e47ba2fac7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = chats[0][0]\n",
    "print(f\"question: {question}\\n\")\n",
    "chat.reply(question)\n",
    "print(chat.last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc2813d8-bad9-47c4-971c-0062a8184fce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Chat with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71aacd6-b7e5-43e3-b124-e4ecff1a6590",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Now let's see what kind of reply we get when we provide context from vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cfb944-ce14-4edd-a161-4b1224af8506",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reset history\n",
    "chat = ChatSession(model=\"databricks-meta-llama-3-70b-instruct\",\n",
    "                   system_message=\"You are a helpful hospital assistant. Answer the user's question based on the provided context.\",\n",
    "                   max_tokens=128)\n",
    "\n",
    "def get_context(*, index, question):\n",
    "    # get context from vector search\n",
    "    raw_context = index.similarity_search(columns=[\"text\", \"title\"],\n",
    "                           query_text=question,\n",
    "                        num_results = 3,\n",
    "                        score_threshold=0.4)\n",
    "    context_string = \"\"\n",
    "    for (i,doc) in enumerate(raw_context.get('result').get('data_array')):\n",
    "        context_string += f\"Retrieved context {i+1}:\\n\"\n",
    "        context_string += doc[0]\n",
    "        context_string += \"\\n\\n\"\n",
    "\n",
    "question = chats[0][0]\n",
    "symptoms_context = get_context(index=symptoms_index, question=question)\n",
    "# procedures_context = get_context(index=procedures_index, question=question)\n",
    "procedures_context = \"\"\n",
    "print(f\"question: {question}\\n\")\n",
    "chat.reply(f\"User question: {question}\\n\\nContext: {symptoms_context}\\n\\n{procedures_context}\")\n",
    "print(chat.last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c06513e-6429-4a01-8019-e774840064cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It is now able to answer based on the provided context.\n",
    "\n",
    "### Congratulations! Demo complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fc2348-7e79-4853-a79e-6eb8938f14d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Additional information\n",
    "\n",
    "## Using the UI\n",
    "Most of the Vector Database management steps above can be done via the UI: you can [create an endpoint](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-a-vector-search-endpoint-using-the-ui), [create an index](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-index-using-the-ui), sync the index, and more via the UI in the Databricks Catalog Explorer.\n",
    "\n",
    "## Experimenting in the AI Playground\n",
    "The [Databricks AI Playground](https://docs.databricks.com/en/large-language-models/ai-playground.html) provides a GUI for quickly experimenting with LLMs available via the FMAPI, enabling you to compare the outputs of those models and determine which model best serves your needs."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2379408111166541,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "specific_health_chat_bot",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
